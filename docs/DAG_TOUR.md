# Logical Dependency Graph Overview

The proof underlying the spectral realization of the Riemann Hypothesis is organized as a **directed acyclic graph (DAG)** of logical dependencies. In this DAG, each node represents a key piece of the proof (definition, lemma, proposition, theorem, etc.), and each directed edge indicates that one result is used in proving another. The entire proof’s logical structure is encoded in this graph, providing a clear, acyclic roadmap from foundational assumptions to the final conclusion. Below we explain what the DAG encodes, why an acyclic graph is crucial for a modular proof, and how it relates to the spectral approach to RH and the project’s reproducibility. We also describe how to navigate the proof via the DAG, how the chapters/files correspond to DAG nodes, and how to maintain the DAG (editing dependencies, checking for cycles, and visualizing the graph).

## What the DAG Encodes: Nodes and Dependencies

**Nodes as Proof Components:** Each node in the DAG corresponds to a specific labeled statement in the proof – for example, a **Definition**, **Lemma**, **Proposition**, **Theorem**, or **Corollary**. These are the building blocks of the theory. Every important assertion that needs proof or is used as a proof ingredient is a node in the graph. For instance, nodes include foundational analytic lemmas (e.g. growth bounds, integrability results), definitions of spectral objects, propositions about operator properties, and theorems linking the spectral construction to the Riemann Hypothesis. The JSON file `dag_nodes.json` lists all nodes with their metadata (type of result, chapter number, and the source file where the content is written). Each node is identified by a unique label like `def:kernel_decay_condition` or `thm:truth_of_rh`, and is associated with a LaTeX source file (e.g. `src/chapters/01_foundations/lems/lem_xi_growth_bound.tex` for a lemma on the growth of Riemann’s xi-function) and a chapter number.

**Edges as Logical Dependencies:** A directed edge from node A to node B means *“A is required to prove B.”* In other words, the result or concept A is used as a prerequisite in the proof of B. For example, if a lemma provides a key estimate that a later proposition relies on, the DAG will have an edge from that lemma’s node to the proposition’s node. **Edges thus capture the flow of logic and information**: definitions needed for a construction, lemmas needed for a theorem, etc. If node X has **no incoming edges**, it means X is a **base assumption or foundational result** that doesn’t depend on any earlier result in this work (for example, an initial definition or an axiom). These “source” nodes represent the starting points of the proof – in a DAG context, they are the “base-level” components. If node Y has no outgoing edges, it’s an **end result** or conclusion (for instance, the final theorem proving RH equivalence). Every edge in `dag_edges.json` is an ordered pair `[A, B]` indicating that **node A is cited in the proof of node B**. By tracing all incoming edges of a node, one can see *exactly* which prior results B depends on; by following outgoing edges, one sees which later results build on A.

**Logical and Spectral Dependencies:** The DAG captures both standard logical dependencies and those specific to the spectral construction in the proof. For example, a node representing an analytic estimate (like a bound on an integral or a function’s growth) may feed into another node representing a spectral result (like a property of the operator \$L\_{\text{sym}}\$). In this way, the graph encodes how **analytic lemmas, spectral lemmas, and number-theoretic criteria all connect**. Each edge represents a *spectral dependency* when it links results in the spectral theory context (e.g. a lemma about an operator’s trace class property feeding into a theorem about the Fredholm determinant), or a *logical dependency* in the usual sense when it links more general results. In practice, there is no sharp division – all edges uniformly show that one established fact is used to establish another. The **full logical dependency structure** of the entire proof is contained in this graph, meaning that if a result is needed later, it will appear as a predecessor in the DAG. Conversely, if something is not proven or cited, it will not appear, ensuring no hidden assumptions.

## Why Use a DAG? Ensuring Acyclic, Modular Dependencies

The use of a directed acyclic graph is a deliberate design choice to enforce **strict logical consistency** and modularity in a very deep proof. The term “acyclic” means there are **no loops**: one cannot start from a node and follow a sequence of dependency edges that eventually leads back to the same node. In terms of logic, this guarantees **no circular reasoning**. No lemma, proposition or theorem ever (directly or indirectly) relies on itself or on any result that is equivalent to it. In particular, **no theorem appeals to any result logically equivalent to the Riemann Hypothesis prior to its proof**, which guarantees *strict acyclicity and transparency*. The DAG structure makes it impossible to smuggle the conclusion (or any equivalent statement) into the proof as a premature assumption – a critical safeguard in a project aimed at a major result like RH.

**Modularity:** Organizing the proof as a DAG also enforces a **modular proof architecture**. Each component of the proof (each node) can be developed and verified independently, as long as its prerequisites (incoming edges) are satisfied. This modularity means the proof is built up in **layers**: one first establishes the base lemmas and definitions (the first layer of nodes with no prerequisites), then builds intermediate results on top of those, and so on, up to the final theorems. The DAG makes these layers explicit – one can, for example, topologically sort the graph to see a valid order in which the results can be introduced without ever using a statement before it is proven. The highest-level “layer” contains the final conclusions (e.g. the theorem proving the equivalence of RH with a spectral condition), and the lowest layer contains foundational definitions/lemmas (e.g. analytic properties of the xi-function, functional equations, etc.). This layering is ideal for analyzing dependencies: it helps identify the **base-level components** of the proof (nodes with no prerequisites) and the **logical depth** of various results (how many layers of lemmas underpin a given theorem). It also makes clear what the **impact of any change** would be – if a foundational lemma is modified, one can immediately see which downstream results (nodes further out in the graph) might be affected.

**Avoiding Logical Cycles:** By insisting on a DAG, the authors ensure the proof has *no strange inter-dependencies* or circular arguments. A directed cycle in a dependency graph would indicate two or more results that rely on each other in a closed loop – a “recipe for strange bugs” in software or a fatal flaw in a mathematical proof. The DAG framework guarantees that **every claim is ultimately grounded in fundamental assumptions or previously proven results**, not in itself. In the context of this project, that means even though the proof is complex, it remains logically sound and **noncircular**. The formal check for acyclicity (discussed under *Maintaining the DAG* below) is an important part of ensuring the integrity of the proof.

**Auditability and Transparency:** A DAG of dependencies greatly improves the **auditability** of the proof. Since each dependency is explicitly documented as an edge, a reviewer or collaborator can trace any given result back to the foundational premises. The structure is **transparent** – one can verify that nothing is assumed without proof, and all cross-references are accounted for. In the authors’ words, the proof is “logically acyclic and audit-transparent”. The DAG acts as a **map for a verifier**: one can start from the final theorem and follow edges backwards to see all the intermediate results required, or start from a fundamental lemma and follow edges forward to see how it is used. This complements the linear narrative of the chapters by providing a bird’s-eye view of the logical architecture.

## Role in the Spectral Proof and Reproducibility

The DAG paradigm is especially valuable given the **spectral nature** of this proof of the Riemann Hypothesis. The project proves RH by constructing a certain operator \$L\_{\text{sym}}\$ whose spectrum encodes the nontrivial zeros of the zeta function. In essence, it establishes that:

* There is a canonical self-adjoint (Hermitian) operator \$L\_{\text{sym}}\$ on a Hilbert space such that its eigenvalues \${\mu\_\rho}\$ correspond exactly to the nontrivial zeros \$\rho\$ of \$\zeta(s)\$ (in fact \$\mu\_\rho = \frac{1}{i}(\rho - \tfrac{1}{2})\$).
* A certain Fredholm determinant associated with \$L\_{\text{sym}}\$ equals the (completed) zeta function \$\Xi(s)\$ up to normalization, linking analytic properties of zeta to spectral properties of the operator.
* **Riemann Hypothesis is then equivalent to the statement that \$L\_{\text{sym}}\$ has a real spectrum** (all \$\mu\_\rho\$ real). This is a spectral reformulation of RH, aligning with the Hilbert–Pólya conjecture idea that RH would follow if one finds a self-adjoint operator whose eigenvalues correspond to the nontrivial zeros.

Proving this equivalence involves a wide range of intermediate results from analysis, operator theory, and number theory. The DAG ensures that the **spectral realization of RH** is achieved in a controlled, verifiable manner. Each spectral construction or claim (such as “\$L\_{\text{sym}}\$ is trace-class and self-adjoint” or “the determinant identity \$\det\_\zeta(I - \lambda L\_{\text{sym}}) = \Xi(1/2 + i\lambda)/\Xi(1/2)\$ holds) is backed by a trail of lemmas and propositions in the graph. By following the DAG, one can see how these pieces come together: for example, foundational lemmas about the \$\Xi\$-function’s Fourier transform and analytic continuation (Chapter 1) feed into the construction of \$L\_{\text{sym}}\$ (Chapter 2), which feeds into proving the Fredholm determinant identity (Chapter 3), which in turn combines with spectral analysis (Chapters 4, 5, 6, etc.) to yield the final equivalence \$RH \Leftrightarrow \Spec(L\_{\text{sym}})\subset \mathbb{R}\$ (Chapter 10). The DAG **highlights this chain of implications clearly**. In particular, it shows that *no part of the spectral argument assumes the truth of RH*; the implication is derived, not assumed.

From a **reproducibility** standpoint, the DAG makes the proof easier to **audit and reproduce** by an independent mathematician. Since every dependency is laid out, one can in principle re-check each node (reproduce each lemma’s proof) knowing exactly what earlier results to use. The DAG essentially provides a **checklist of prerequisites for each proof**. This is invaluable in a complex proof: it prevents subtle errors where a proof might accidentally rely on an unproven fact. The authors explicitly note that the proof is “strictly modular and acyclic” and that *all* needed estimates, identities, and implications are either proven explicitly or cited with proof in the text. The DAG data structure is the mechanism by which this rigor is enforced. It contributes to reproducibility by making the structure of the proof *machine-checkable*: one can run an automated check (via `make check`) to verify that every dependency edge points to an existing node and that no cycles exist (no logical inconsistency). Thus, not only is the proof logically sound, but the structure itself is **documented and checkable** by computer tools. This level of formal structuring is akin to the rigor in formal proof verification systems, but done in a lightweight way using the DAG.

Moreover, the DAG promotes **collaborative reproducibility**: if multiple authors or reviewers are involved, they can split work by following the graph. For example, one person can verify all nodes in one subgraph (say all lemmas about the heat kernel asymptotics in Chapter 5) and be confident that this covers everything needed for any theorem depending on those lemmas. If any part of the proof were to be modified or improved (say a lemma’s proof updated for clarity or a step optimized), the DAG helps isolate the impact of that change. By checking outgoing edges from that lemma, one immediately knows which later results might need a review for consistency. This is analogous to seeing which later components in a software project depend on a given module.

In summary, the DAG ensures the **spectral proof of RH is rigorous, transparent, and reproducible**. It aligns with the philosophy of the Hilbert–Pólya approach by cleanly separating the assumption and conclusion: all the analytic and spectral groundwork is laid out without assuming RH, and only at the end is RH *derived* as an equivalence from the cumulative results. The graph explicitly shows this flow, making it easier to trust and verify each step of the spectral realization program.

## Navigating the Proof via Dependency Chains

One of the practical benefits of the DAG is that it helps **navigate the deep proof architecture** by clarifying dependency chains. A proof of this scope (spanning ten chapters, numerous appendices, and many interlocking results) can be daunting to read linearly. The DAG serves as a roadmap or **“proof flow diagram”** that readers can consult to understand how different parts of the argument connect.

For example, if a reader is interested in how the final result (RH equivalence) is obtained, they can start at the node for the final theorem (in Chapter 10) and trace backwards through its prerequisites: perhaps it depends on a certain theorem in Chapter 9 and a proposition in Chapter 7; those in turn depend on the determinant identity from Chapter 3, some spectral gap lemma from Chapter 8, and a Tauberian argument from Chapter 7, and so on. By following these arrows backward, one uncovers the **hierarchy of results** leading to the final conclusion. This can be more informative than a simple linear reading because it organizes the reasoning by logical dependence rather than by chapter order. The DAG graph can even be visualized (using the provided `dag_visualizer.py` tool) to produce a schematic like **Figure 2 in Appendix B**, which shows an overview of dependencies between chapters. In that high-level view, each chapter is a node, and arrows show which chapters rely on results from which others. Arrows “flow” mostly from earlier to later chapters, reflecting that Chapter 1’s results support Chapter 2, and so on – but the few exceptions (where a later chapter’s result is used forward in an earlier chapter) are also indicated, with no cycles present.

By clarifying these dependency chains, the DAG helps a reader answer questions like: *“What preliminary results do I need to understand before reading the proof of this theorem?”* or *“Where is this lemma eventually used?”* For instance, a reader encountering an analytic lemma about the \$\Xi(s)\$ function in Chapter 1 can look at the DAG and see that this lemma is the **root** (a prerequisite for many subsequent nodes) – in fact, the exponential weight bound Lemma 1.24 is identified as the **root node of the entire DAG** because it provides a critical condition (\$\alpha > \pi\$) that underpins the trace-class construction of \$L\_{\text{sym}}\$. Knowing this, the reader appreciates the lemma’s importance and can anticipate seeing it cited in multiple places. On the other hand, if one looks at a mid-level node, like *Theorem 4.10 (Spectral Bijection)* which asserts a one-to-one correspondence between zeros of zeta and eigenvalues of \$L\_{\text{sym}}\$, the DAG would show exactly which lemmas and propositions feed into its proof (for example, likely the determinant identity from Chapter 3, some functional analytic lemmas from Chapter 2, etc.). This **clear tracing of prerequisites** helps to compartmentalize the proof: you can study one subchain at a time, confident that when you move to another part, you won’t unexpectedly need an external fact that wasn’t flagged.

Additionally, the DAG aids in navigating **forward references**. In a traditional paper, if a result from a later section is used early (a forward reference), it can confuse readers. Here, any such forward dependency is explicitly marked in the DAG (and in the text via remarks). The authors have a concept of “postulated forward dependency” which they document (e.g., Remark 3.25 in the text declares when Chapter 3 temporarily relies on a result proved in Chapter 5, as noted in Appendix B). The DAG includes these edges (from the later result back to the earlier usage) but, crucially, because the graph is acyclic, such a forward reference does *not* create a cycle – it will eventually loop back through the final equivalence to close the reasoning, but only after the forward-pointing result is proven. The DAG helps readers **navigate these forward references without confusion**, since one can see that an edge comes from a higher-numbered chapter and understand it as a deliberate, checked step that will be resolved later. This way, the DAG presentation keeps the proof *readable and structured* while still allowing occasional out-of-order usage in a controlled manner (and always resolving it later so that no circular logic occurs).

In summary, using the DAG to navigate the proof means you have a **guide to the proof’s structure**. It provides clarity on *which results depend on which others*. This is extremely helpful for a general mathematical audience: even if one is not an expert in spectral theory, the dependency graph itself is a familiar concept (akin to an outline or flowchart of a proof). By consulting the DAG, a reader can approach the proof in a logical order, ensure they have covered all necessary background results before tackling a complex theorem, and appreciate how various sub-results contribute to the overall argument. It transforms a 200-page deep technical manuscript into a **comprehensible network of logical implications** that can be followed step by step.

## Mapping Chapters and Files to DAG Nodes

The formal write-up of the proof is divided into chapters and appendices, and the DAG’s nodes are aligned with this structure. Each chapter (and some appendices containing technical refinements) corresponds to a group of nodes in the DAG. The mapping works as follows:

* **Chapters as Collections of Nodes:** Each chapter focuses on a thematic part of the proof and contains several labeled results (definitions, lemmas, theorems, etc.). For example, Chapter 1 (Foundational Structures) introduces basic definitions and lemmas (nodes) about the Riemann xi-function, Hilbert spaces, and integral kernels. Chapter 2 (Operator Construction) contains nodes that construct and establish properties of the operator \$L\_{\text{sym}}\$ (definitions of the operator and lemmas proving it is compact, self-adjoint, etc.). Chapter 3 (Determinant Identity) has nodes for the Fredholm determinant identity and supporting propositions. Chapter 4 (Spectral Encoding) includes theorems and corollaries that the spectrum of \$L\_{\text{sym}}\$ corresponds to zeta zeros. And so on up to Chapter 10, which contains the final equivalences and “logical closure” of the proof. Each such result in the text is a node in the DAG, labeled by chapter. The DAG in Appendix B even shows arrows between entire **chapters** to summarize high-level dependencies – for instance, an arrow from Chapter 5 to Chapter 3 in that figure indicates Chapter 3’s main result relied on something proven in Chapter 5 (a forward dependency that was later resolved).

* **File Organization:** In the project repository, each result is written in its own TeX file, and these files are organized by chapter and by type. For instance, files for lemmas in Chapter 1 reside in `src/chapters/01_foundations/lems/`, definitions in `src/chapters/01_foundations/defs/`, etc. The JSON file `dag_nodes.json` explicitly maps each node label to its chapter number and file path. For example, a node labeled `"lem:xi_growth_bound"` might have an entry:

  ```json
  "lem:xi_growth_bound": {
      "chapter": 1,
      "file": "src/chapters/01_foundations/lems/lem_xi_growth_bound.tex",
      "type": "lemma",
      "hash": "…"
  },
  ```

  This tells us that the lemma *“Xi Growth Bound”* is proven in Chapter 1 and the content is in the file `lem_xi_growth_bound.tex`. Similarly, a theorem node like `"thm:truth_of_rh"` (the final theorem establishing the truth of the Riemann Hypothesis under the spectral criterion) might be mapped to Chapter 10 and a file `src/chapters/10_logical_closure/thms/thm_truth_of_rh.tex`. The **chapter number in the node metadata corresponds to the chapter in which the result is presented**, and the file path allows you to locate the full statement and proof text.

* **Labels and LaTeX cross-references:** The labels used as node IDs (like `lem:...`, `thm:...`) are also the labels used in the LaTeX source for cross-referencing. This means that whenever the text says “Lemma 1.24” or refers to a result by name, it is likely tagged with a `\label{lem:xi_growth_bound}` (for example) in the source. These labels are what the DAG uses to identify nodes. Thus, the DAG is tightly integrated with the document: if a proof cites “Lemma 2.14,” the LaTeX will reference that label, and the `dag_edges.json` will have an entry linking the corresponding node as a prerequisite. This ensures **consistency** between the written document and the dependency graph – if you update a label or add a new one, it should be reflected in the DAG files as well.

* **Chapters as Subgraphs:** One way to view the structure is that *each chapter roughly corresponds to a subgraph of the DAG*, containing all nodes with that chapter number. Within a chapter, results typically build on earlier results of the same chapter or previous chapters. Indeed, the chapters were written in a sequence that mostly respects the partial order of the DAG (Chapter 1’s results come first, then Chapter 2, etc., so that usually a result’s prerequisites have appeared in an earlier chapter). However, as noted, there are a few intentional forward links. The DAG makes it clear which those are, and the text flags them so readers know that, say, a Chapter 3 argument uses a result whose proof appears in Chapter 5, but no logical inconsistency is present because ultimately Chapter 5 does not depend on Chapter 3 (hence no cycle).

* **Example Mapping:** To make this concrete, consider a specific example: *Lemma 2.9* might be “\$L\_t\$ converges in trace norm to \$L\_{\text{sym}}\$” (a hypothetical statement in the operator construction chapter). In the DAG, it could be labeled `lem:trace_norm_convergence_Lt_to_Lsym` with `"chapter": 2` and a file path in `02_operator_construction/lems/lem_trace_norm_convergence_Lt_to_Lsym.tex`. This lemma may depend on some Chapter 1 lemmas (e.g. an integral bound) – so in `dag_edges.json` we might see entries like `["lem:xi_growth_bound", "lem:trace_norm_convergence_Lt_to_Lsym"]` indicating that the xi-function growth bound from Chapter 1 is used in the proof of this convergence lemma. We might also see a dependency on another Chapter 2 lemma (e.g. `["lem:kernel_L2_weighted_bound", "lem:trace_norm_convergence_Lt_to_Lsym"]` if that was something established just prior in Chapter 2). By looking at the DAG entries, one can reconstruct this mapping of content: **chapter 1 lemmas → chapter 2 lemmas → chapter 2 propositions → chapter 3 theorems**, etc., matching the narrative structure.

The takeaway is that the DAG is not an abstract overlay – it is directly connected to the document structure. The chapters provide a human-readable **linear order** to follow, while the DAG provides the **underlying dependency graph** that might not be strictly linear. The files in `src/chapters/` are organized by chapter and by result type largely to mirror the DAG nodes. When adding or modifying content, developers update these files and then update the DAG data accordingly, keeping the two in sync.

## Editing and Visualizing the DAG

Maintaining the DAG is an important part of the project’s workflow. As new results are added or existing ones are changed, the DAG (in the JSON files) must be kept up-to-date to reflect the correct dependencies. The project provides tools for editing and verifying the DAG structure:

* **Adding or Removing Nodes:** To introduce a new lemma, theorem, or other result, you should create a new labeled environment in the LaTeX source (in the appropriate chapter file or a new file). Then, add an entry for this node in `dag_nodes.json`. The entry should include the node’s label (which matches the LaTeX label), the chapter number, the file path, and the type (e.g. “lemma”). This ensures the node is recognized as part of the DAG. If a result is removed or its label is changed, the `dag_nodes.json` must be updated accordingly (delete or modify the old entry).

* **Editing Dependencies (dag\_edges.json):** Whenever you add a new result or change how proofs are structured, you must update `dag_edges.json` to reflect the new dependencies. Each line in `dag_edges.json` is a pair `["A", "B"]` meaning node A is used in node B’s proof. If your new lemma (call it L\_new) relies on two existing lemmas L1 and L2, you would add entries:

  ```json
  ["lem:L1", "lem:L_new"],
  ["lem:L2", "lem:L_new"],
  ```

  Similarly, if a new theorem uses that lemma, add `["lem:L_new", "thm:new_theorem"]`. It’s important to list **all** direct dependencies that appear in the proofs. Typically, this corresponds to citations in the text: if in the proof of Theorem X you cite Lemma Y and Proposition Z, then edges (Y → X) and (Z → X) should be in the DAG. By updating these edges diligently, you keep the graph accurate. If a dependency is removed (perhaps a proof is refactored to no longer need a certain lemma), you should remove that edge from the JSON as well.

* **Verifying Acyclicity (`make check`):** To ensure that the DAG remains acyclic and consistent, the project includes a check (often invoked via a Makefile target, e.g. running `make check`). This will likely run a script that reads `dag_nodes.json` and `dag_edges.json` and performs validation:

  * It checks that every edge refers to valid node labels (catching typos or missing node definitions).
  * It then checks for cycles in the directed graph. This is usually done by attempting a topological sort or using an algorithm to detect cycles. If a cycle is found, the check will report an error (and no cycle should ever be present in a correct proof dependency graph).
  * It might also check for other inconsistencies (like a node marked in a later chapter depending on one marked in a higher-numbered chapter without the forward-dependency being allowed, though formally that’s just another cycle check).

  You should run this check after any edits to the DAG. If `make check` reports a cycle, it means a mistake has been made in the dependencies. You’ll need to examine the reported cycle and adjust edges or the proof structure to resolve it. For example, a cycle could occur if someone inadvertently made two results depend on each other (perhaps by citing each other in a circular way); the fix might be to remove one citation or break the result into separate lemmas that are truly acyclic. Passing the acyclicity check gives confidence that the **logical structure is “sound and noncircular”**.

* **Visualizing the DAG (`dag_visualizer.py`):** To get a clearer picture of the dependency graph, you can use the provided Python script `dag_visualizer.py`. This script reads the JSON files and produces a visual representation of the graph. The output may be, for example, a PDF or image showing nodes as points (with their labels or an abbreviated form) and arrows for each dependency. This can be useful for documentation (indeed, the figure in Appendix B was likely produced this way) or simply for understanding the proof structure. You might invoke it with a command like `python dag_visualizer.py dag_nodes.json dag_edges.json` or via a Makefile target. The visualizer could allow filtering or simplifying the graph – for instance, showing a high-level view by collapsing nodes by chapter, or highlighting the path to a particular theorem.

  When using the visualizer, ensure you have the necessary libraries (perhaps Graphviz or networkx) installed, as it may generate a Graphviz DOT file or use a library to draw the graph. The result is often a diagram where one can see, say, Chapter 1 nodes feeding into Chapter 2 nodes, etc., with no cycles. **Figure 2** in the documentation is an example of a chapter-level DAG view. For an internal detailed view, the full DAG might be quite large, but still invaluable for tracking complex dependencies. If the visual graph is too cluttered, consider generating subgraphs (e.g., focusing on a subset of chapters or one particular thread of the proof). The DAG visualizer may have options to do this.

* **Best Practices in Editing:** When editing the DAG, maintain consistency:

  * Keep node labels unique and descriptive (they usually are mnemonic, like `lem:kernel_symmetry` for a lemma about kernel symmetry).
  * Update the chapter number if you happen to move a result from one chapter to another.
  * It’s often helpful to keep the `dag_nodes.json` sorted (perhaps by chapter or alphabetically by label) so it's easy to find entries.
  * For `dag_edges.json`, since it’s just a list of pairs, you might group edges by the target node or source node for readability, but the order doesn’t matter for the program. Comments are not in JSON, but you can regenerate or format it via scripts if needed.
  * After editing edges, run the check to catch errors early. If a cycle is reported, examine the involved nodes: it might be that a forward reference was introduced without marking it properly, or a proof was rearranged in a problematic way. Remember that **forward dependencies are allowed only if they don’t create a cycle**; in practice, this means you can cite something ahead only if the chain of reasoning eventually comes back around *after* that thing is proven (closing the loop at the end, not in a circle). The DAG check will prevent any genuine circular logic.

By following these steps and using the available tools, one can **safely extend or modify the proof** while preserving its logical integrity. The DAG effectively acts as both a **blueprint and a safeguard**: it guides where new pieces should connect, and it flags any structural issues that could compromise the proof. This ensures the project remains **robust, up-to-date, and accessible** to those who wish to understand or verify it.

**In summary**, the Directed Acyclic Graph of lemmas, theorems, and definitions is the backbone of the project’s proof structure. It encodes the logical flow in a way that is **transparent, checkable, and pedagogically helpful**. By adhering to an acyclic graph of dependencies, the authors ensure the proof of the Riemann Hypothesis via a spectral approach is both **rigorously correct** and **navigable** by others. The DAG highlights the modular nature of the proof, where each result builds on earlier ones without circularity, ultimately leading to the spectacular conclusion that the reality of the spectrum of a certain operator is equivalent to the truth of RH. This approach not only aids in proving such a deep result but also sets a standard for *reproducible and auditable mathematics*, where complex proofs can be unfolded and understood through their dependency graphs.
